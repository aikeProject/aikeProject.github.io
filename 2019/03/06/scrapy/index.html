<!DOCTYPE HTML><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta http-equiv="Cache-Control" content="no-siteapp"><meta http-equiv="Cache-Control" content="no-transform"><meta name="renderer" content="webkit|ie-comp|ie-stand"><meta name="apple-mobile-web-app-capable" content="成雨的个人博客"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="browsermode" content="application"><meta name="screen-orientation" content="portrait"><link rel="dns-prefetch" href="http://blog.rensailong.top"><meta name="keywords" content="爬虫,python,scrapy"><meta name="description" content="官方地址安装1conda install scrapy
创建一个项目
scrapy startproject myproject [project_dir]
cd project_dir
scr..."><meta name="robots" content="all"><meta name="google" content="all"><meta name="googlebot" content="all"><meta name="verify" content="all"><title>scrapy | 成雨的个人博客</title><link rel="alternate" href="/atom.xml" title="成雨的个人博客" type="application/atom+xml"><link rel="icon" href="/favicon-20190220051001892.ico"><link rel="stylesheet" href="/css/bootstrap.min.css?rev=0ee553ffb8c61021be4ac517c25c07b5"><link rel="stylesheet" href="/css/font-awesome.min.css?rev=afecc2c74d492b747ec32ef68e9d06ec"><link rel="stylesheet" href="/css/style.css?rev=20c56e3d4978a648b6298e44bd3ae711"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?ccd98c2d3b145ffad4c50be87f40400b";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><meta name="baidu-site-verification" content="Ic3zHIPe6C"><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script></head></html><!--[if lte IE 8]><style>
    html{ font-size: 1em }
</style><![endif]--><!--[if lte IE 9]><div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div><![endif]--><body><header class="main-header" style="background-image:url(/./img/my-banner.jpg)"><div class="main-header-box"><a class="header-avatar" href="/" title="成雨"><img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block"></a><div class="branding"><h2>你若化成风，我便化成雨</h2></div></div></header><nav class="main-navigation"><div class="container"><div class="row"><div class="col-sm-12"><div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav"><span class="sr-only"></span><i class="fa fa-bars"></i></span> <a class="navbar-brand" href="http://blog.rensailong.top">成雨的个人博客</a></div><div class="collapse navbar-collapse" id="main-menu"><ul class="menu"><li role="presentation" class="text-center"><a href="/"><i class="fa"></i> 首页</a></li><li role="presentation" class="text-center"><a href="/categories/前端/"><i class="fa"></i> 前端</a></li><li role="presentation" class="text-center"><a href="/categories/后端/"><i class="fa"></i> 后端</a></li><li role="presentation" class="text-center"><a href="/categories/工具/"><i class="fa"></i> 工具</a></li><li role="presentation" class="text-center"><a href="/archives/"><i class="fa"></i> 时间轴</a></li></ul></div></div></div></div></nav><section class="content-wrap"><div class="container"><div class="row"><main class="col-md-8 main-content m-post"><p id="process"></p><article class="post"><div class="post-head"><h1 id="scrapy">scrapy</h1><div class="post-meta"><span class="categories-meta fa-wrap"><i class="fa fa-folder-open-o"></i> <a class="category-link" href="/categories/后端/">后端</a></span><span class="fa-wrap"><i class="fa fa-tags"></i> <span class="tags-meta"><a class="tag-link" href="/tags/python/">python</a> <a class="tag-link" href="/tags/scrapy/">scrapy</a> <a class="tag-link" href="/tags/爬虫/">爬虫</a></span></span><span class="fa-wrap"><i class="fa fa-clock-o"></i> <span class="date-meta">2019/03/06</span></span><span class="fa-wrap"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="post-body post-content"><h3 id="官方地址"><a href="#官方地址" class="headerlink" title="官方地址"></a><a href="https://docs.scrapy.org/en/latest/intro/tutorial.html" target="_blank" rel="noopener">官方地址</a></h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install scrapy</span><br></pre></td></tr></table></figure><h4 id="创建一个项目"><a href="#创建一个项目" class="headerlink" title="创建一个项目"></a>创建一个项目</h4><ol><li><code>scrapy startproject myproject [project_dir]</code></li><li><code>cd project_dir</code></li><li><code>scrapy genspider mydomain mydomain.com</code></li></ol><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><ul><li>通过<code>scrapy genspider zhihu www.zhihu.com</code>在<code>spiders</code>文件夹下生产爬取逻辑的代码文件</li><li><p>通过例子看基本使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># zhihu.py 通过 scrapy genspider zhihu www.zhihu.com 创建的</span></span><br><span class="line"><span class="comment"># 写主要的爬虫逻辑</span></span><br><span class="line"><span class="comment"># 这个例子是爬取知乎用户的关注人，关注人的关注人，一直递归下去</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> UserItem</span><br><span class="line"></span><br><span class="line"><span class="comment"># 爬取知乎需要添加一些请求头信息</span></span><br><span class="line"><span class="comment"># 打开setting.py中的 DEFAULT_REQUEST_HEADERS并添加user-agent</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'zhihu'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.zhihu.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.zhihu.com/'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始爬取的用户token_url，看一下知乎的请求链接结构就知道这个是用来干啥的了，其实就是用来标识每一个用户</span></span><br><span class="line">    start_user = <span class="string">'Talyer-Wei'</span></span><br><span class="line">    <span class="comment"># 请求用户详细信息</span></span><br><span class="line">    user_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 请求关注用户列表</span></span><br><span class="line">    follows_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这个是粉丝列表请求，爬取和爬关注用户是一样逻辑，没写</span></span><br><span class="line">    <span class="comment"># followers_url = 'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followers?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># follows_url中include的参数</span></span><br><span class="line">    user_query = <span class="string">'allow_message%2Cis_followed%2Cis_following%2Cis_org%2Cis_blocking%2Cemployments%2Canswer_count%2Cfollower_count%2Carticles_count%2Cgender%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># follows_url中include的请求参数</span></span><br><span class="line">    follows_query = <span class="string">'data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写启动请求</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 爬取个人信息</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">            self.user_url.format(</span><br><span class="line">                user=self.start_user,</span><br><span class="line">                include=self.user_query</span><br><span class="line">            ),</span><br><span class="line">            callback=self.parse_user</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 爬取初始的关注用户</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">            self.follows_url.format(</span><br><span class="line">                user=self.start_user,</span><br><span class="line">                include=self.follows_query,</span><br><span class="line">                offset=<span class="number">0</span>,</span><br><span class="line">                limit=<span class="number">20</span></span><br><span class="line">            ),</span><br><span class="line">            callback=self.parse_follows</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 请求回调</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_user</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        result = json.loads(response.text)</span><br><span class="line"></span><br><span class="line">        item = UserItem()</span><br><span class="line">        <span class="keyword">for</span> field <span class="keyword">in</span> item.fields:</span><br><span class="line">            <span class="keyword">if</span> field <span class="keyword">in</span> result.keys():</span><br><span class="line">                item[field] = result[field]</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 爬取个人的详细信息</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">            self.follows_url.format(</span><br><span class="line">                user=result.get(<span class="string">'url_token'</span>),</span><br><span class="line">                include=self.follows_query,</span><br><span class="line">                offset=<span class="number">0</span>,</span><br><span class="line">                limit=<span class="number">20</span></span><br><span class="line">            ),</span><br><span class="line">            callback=self.parse_follows</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬取关注自己的人</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_follows</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        results = json.loads(response.text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 再递归爬取关注自己人的，他的关注信息，就这样一直递归下去</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'data'</span> <span class="keyword">in</span> results.keys():</span><br><span class="line">            <span class="keyword">for</span> result <span class="keyword">in</span> results.get(<span class="string">'data'</span>):</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">                    url=self.user_url.format(</span><br><span class="line">                        user=result.get(<span class="string">'url_token'</span>),</span><br><span class="line">                        include=self.user_query</span><br><span class="line">                    ),</span><br><span class="line">                    callback=self.parse_user</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分页还未结束继续查找分页</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'paging'</span> <span class="keyword">in</span> results.get(<span class="string">'paging'</span>) <span class="keyword">and</span> results.get(<span class="string">'paging'</span>).get(<span class="string">'is_end'</span>) == <span class="literal">False</span>:</span><br><span class="line">            next_page = results.get(<span class="string">'paging'</span>).get(<span class="string">'next'</span>)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">                url=next_page,</span><br><span class="line">                callback=self.parse_follows</span><br><span class="line">            )</span><br></pre></td></tr></table></figure></li><li><p>定义 <code>items</code>，相当于要保存数据的模型，通过<code>items.py</code>配置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://doc.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserItem</span><span class="params">(Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    id = Field()</span><br><span class="line">    name = Field()</span><br><span class="line">    avatar_url = Field()</span><br><span class="line">    headline = Field()</span><br><span class="line">    description = Field()</span><br><span class="line">    url = Field()</span><br><span class="line">    url_token = Field()</span><br><span class="line">    gender = Field()</span><br><span class="line">    cover_url = Field()</span><br><span class="line">    type = Field()</span><br><span class="line">    badge = Field()</span><br><span class="line"></span><br><span class="line">    answer_count = Field()</span><br><span class="line">    articles_count = Field()</span><br><span class="line">    commercial_question_count = Field()</span><br><span class="line">    favorite_count = Field()</span><br><span class="line">    favorited_count = Field()</span><br><span class="line">    follower_count = Field()</span><br><span class="line">    following_columns_count = Field()</span><br><span class="line">    following_count = Field()</span><br><span class="line">    pins_count = Field()</span><br><span class="line">    question_count = Field()</span><br><span class="line">    thank_from_count = Field()</span><br><span class="line">    thank_to_count = Field()</span><br><span class="line">    thanked_count = Field()</span><br><span class="line">    vote_from_count = Field()</span><br><span class="line">    vote_to_count = Field()</span><br><span class="line">    voteup_count = Field()</span><br><span class="line">    following_favlists_count = Field()</span><br><span class="line">    following_question_count = Field()</span><br><span class="line">    following_topic_count = Field()</span><br><span class="line">    marked_answers_count = Field()</span><br><span class="line">    mutual_followees_count = Field()</span><br><span class="line">    hosted_live_count = Field()</span><br><span class="line">    participated_live_count = Field()</span><br><span class="line"></span><br><span class="line">    locations = Field()</span><br><span class="line">    educations = Field()</span><br><span class="line">    employments = Field()</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></li><li><p><code>pipelines.py</code> 在这个文件夹中，将<code>items</code>存入数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># Define your item pipelines here</span><br><span class="line">#</span><br><span class="line"># Don&apos;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="line"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># class TutorialPipeline(object):</span><br><span class="line">#     def process_item(self, item, spider):</span><br><span class="line">#         return item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import pymongo</span><br><span class="line"># 保存到MongoDB数据库</span><br><span class="line">class MongoPipeline(object):</span><br><span class="line"></span><br><span class="line">    collection_name = &apos;users&apos;</span><br><span class="line"></span><br><span class="line">    # 初始化数据库</span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    #获取全局setting设置</span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),</span><br><span class="line">            mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    # 连接数据库</span><br><span class="line">    # open_spider spider 启动</span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    # close_spider 关闭</span><br><span class="line">    # 关闭数据库链接</span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    # 存入数据</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # 更新去重</span><br><span class="line">        self.db[self.collection_name].update(&#123;&apos;url_token&apos;: item[&apos;url_token&apos;]&#125;, dict(item), True)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure></li></ul></div><div class="post-footer"><div>转载声明：商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="//github.com/aikeProject" target="_blank">Snippet</a></div><div></div></div></article><div class="article-nav prev-next-wrap clearfix"><a href="/2019/03/06/beautifulsoup/" class="pre-post btn btn-default" title="beautifulsoup"><i class="fa fa-angle-left fa-fw"></i> <span class="hidden-lg">上一篇</span> <span class="hidden-xs">beautifulsoup</span></a> <a href="/2019/03/05/gulp/" class="next-post btn btn-default" title="gulp"><span class="hidden-lg">下一篇</span> <span class="hidden-xs">gulp</span><i class="fa fa-angle-right fa-fw"></i></a></div><div id="comments"><div id="vcomments" class="valine"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="/assets/valine.min.js"></script><script>new Valine({av:AV,el:"#vcomments",appId:"4fxhv6ljJpn9Ri1OJgXCMQCH-gzGzoHsz",appKey:"ef2FdLkY74Ypgpu6k9FDcGep",placeholder:"畅所欲言",notify:!1,verify:!1,avatar:"mm",meta:"nick,mail".split(","),pageSize:"10",path:window.location.pathname,lang:"zh-CN".toLowerCase()})</script></div></main><aside id="article-toc" role="navigation" class="col-md-4"><div class="widget"><h3 class="title">文章目录</h3><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#官方地址"><span class="toc-text">官方地址</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#安装"><span class="toc-text">安装</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#创建一个项目"><span class="toc-text">创建一个项目</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#基本使用"><span class="toc-text">基本使用</span></a></li></ol></li></ol></div></aside></div></div></section><footer class="main-footer"><div class="container"><div class="row"></div></div></footer><a id="back-to-top" class="icon-btn hide"><i class="fa fa-chevron-up"></i></a><div class="copyright"><div class="container"><div class="row"><div class="col-sm-12"><div class="busuanzi">访问量:<strong id="busuanzi_value_site_pv"><i class="fa fa-spinner fa-spin"></i></strong> &nbsp; | &nbsp; 访客数:<strong id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></strong></div></div><div class="col-sm-12"><span>Copyright &copy; 2017</span> | <span>Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a></span> | <span>Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a></span></div></div></div></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/app.js?rev=586c04b4e85931a49156b4ee90d1d3b6"></script></body>